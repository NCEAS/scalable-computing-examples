{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Objectives\n",
        "\n",
        "- Understand what parallel computing is and when it may be useful\n",
        "- Understand how parallelism can work\n",
        "- Review sequential loops and map functions\n",
        "- Build a parallel program using `concurrent.futures`\n",
        "- Build a parallel program using `parsl`\n",
        "- Understand Thread Pools and Process pools\n",
        "\n",
        "## Parallel processing in the shell\n",
        "\n",
        "Shell programming helps massively speed up data management tasks, and even more so with simple use of the [GNU `parallel`](https://www.gnu.org/software/bash/manual/html_node/GNU-Parallel.html) utility to execute bash commands in parallel. In its simplest form, this can be used to speed up common file operations, such as renaming, compression, decompression, and file transfer. Let's look at a common example -- calculating checksums to verify file integrity for data files. Calculating a hash checksum using the `shasum` command can be time consuming, especially when you have a lot of large files to work on. But it is a classic processor-limited task, and one that can be massively faster using `parallel`. \n",
        "\n",
        "First open a Bash Terminal, and then use \"Split terminal\" to create another, side-by-side. In the left terminal, run `htop`.\n",
        "\n",
        "Now maximize your terminal, and in the right terminal run some `parallel` commands.\n",
        "\n",
        "```bash\n",
        "$ cd /home/shares/example-pdg-data\n",
        "$ for fn in `ls *.gpkg`; do shasum -a 256 ${fn}; done\n",
        "\n",
        "real\t35.081s\n",
        "user\t32.745s\n",
        "system\t2.336s\n",
        "\n",
        "$ ls *.gpkg | parallel \"shasum -a 256 {}\"\n",
        "\n",
        "real    2.97s \n",
        "user    37.16s \n",
        "system  2.70s\n",
        "```\n",
        "\n",
        "The first invocation takes *35 seconds* to execute the tasks one at a time serially, while the second version only takes :tada: *3 seconds* :tada: to do the same tasks. Note that the computational time spent in `user` and `system` processing is about the same, with the major difference being that the user-space tasks were conducted on multiple cores in parallel, resulting in more than 10x faster performance, Using `htop`, you can see processor cores spiking in usage when the command is run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task parallelization in Python\n",
        "\n",
        "Python also provides a number of easy to use packages for concurrent processing. We will review two of these, `concurrent.futures` and `parsl`, to show just how easy it can be to parallelize your programs. [`concurrent.futures`](https://docs.python.org/3/library/concurrent.futures.html) is built right into the python3 release, and is a great starting point for learning concurrency.\n",
        "\n",
        "FIrst, we'll get set up by creating a small function for timing other functions, for use as a [decorator](https://realpython.com/primer-on-python-decorators/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def timethis(func):\n",
        "    \"\"\" \n",
        "    Print the execution time for a function call\n",
        "    \"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapped_method(*args, **kwargs):\n",
        "        time_start = time.time()\n",
        "        output = func(*args, **kwargs)\n",
        "        time_end = time.time()\n",
        "        print(f\"{func.__name__}: {(time_end-time_start)*1000} ms\")\n",
        "\n",
        "        return output\n",
        "\n",
        "    return wrapped_method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to start with a task that is a little expensive to compute, and define it in a function. All this `task(x)` function does is to use numpy to create a fairly large range of numbers, and then sum them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def task(x):\n",
        "    import numpy as np\n",
        "    result = np.arange(x*10**8).sum()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can start by executing this task function serially ten times with varying inputs. In this case, we create a function `run_serial` that takes a list of inputs to be run, and it calls the `task` function for each of those inputs. The `@timethis` decorator is a simple way to wrap the function with timing code so that we can see how long it takes to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_serial: 15736.198902130127 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[np.int64(0),\n",
              " np.int64(4999999950000000),\n",
              " np.int64(19999999900000000),\n",
              " np.int64(44999999850000000),\n",
              " np.int64(79999999800000000),\n",
              " np.int64(124999999750000000),\n",
              " np.int64(179999999700000000),\n",
              " np.int64(244999999650000000)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "@timethis\n",
        "def run_serial(task_list):\n",
        "    return [task(x) for x in task_list]\n",
        "\n",
        "run_serial(np.arange(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, it takes around *25 seconds* to execute 10 tasks, depending on what else is happening on the machine and network.\n",
        "\n",
        "So, can we make this faster using a multi-threaded parallel process? Let's try with `concurrent.futures`. The main concept in this package is one of a `future`, which is a structure which represents the value that will be created in a computation in the future when the function completes execution. With `concurrent.futures`, tasks are scheduled and do not block while they await their turn to be executed. Instead, threads are created and executed *asynchronously*, meaning that the function returns it's `future` potentially before the thread has actually been executed. Using this approach, the user schedules a series of tasks to be executed asynchronously, and keeps track of the futures for each task. When the future indicates that the execution has been completed, we can then retrieve the result of the computation.\n",
        "\n",
        "In practice this is a simple change from our serial implementation. We will use the `ThreadPoolExecutor` to create a pool of workers that are available to process tasks. Each worker is set up in its own thread, so it can execute in parallel with other workers. After setting up the pool of workers, we use concurrent.futures `map()` to schedule each task from our `task_list` (in this case, an input value from 1 to 10) to run on one of the workers. As for all map() implementations, we are asking for each value in `task_list` to be executed in the `task` function we defined above, but in this case it will be executed using one of the workers from the `executor` that we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_threaded: 8709.380388259888 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[np.int64(0),\n",
              " np.int64(4999999950000000),\n",
              " np.int64(19999999900000000),\n",
              " np.int64(44999999850000000),\n",
              " np.int64(79999999800000000),\n",
              " np.int64(124999999750000000),\n",
              " np.int64(179999999700000000),\n",
              " np.int64(244999999650000000)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "@timethis\n",
        "def run_threaded(task_list):\n",
        "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        return executor.map(task, task_list)\n",
        "\n",
        "results = run_threaded(np.arange(8))\n",
        "[x for x in results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This execution took about *4 seconds*, which is about 6.25x faster than serial. Congratulations, you wrote your a multi-threaded python program!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Parallel downloads\n",
        "\n",
        "In this exercise, we're going to parallelize a simple task that is often very time consuming -- downloading data. And we'll compare performance of simple downloads using first a serial loop, and then using two parallel execution libraries: `concurrent.futures` and `parsl`. We're going to see an example here where parallel execution won't always speed up this task, as this is likely an I/O bound task if you're downloading a lot of data. But we still should be able to speed things up a lot until we hit the limits of our disk arrays.\n",
        "\n",
        "The data we are downloading is a pan-Arctic time series of TIF images containing rasterized Arctic surface water indices from:\n",
        "\n",
        "> Elizabeth Webb. 2022. Pan-Arctic surface water (yearly and trend over time) 2000-2022. Arctic Data Center [doi:10.18739/A2NK3665N](https://doi.org/doi:10.18739/A2NK3665N). \n",
        "\n",
        "First, let's download the data serially to set a benchmark. The data files are listed in a table with their filename and identifier, and can be downloaded directly from the Arctic Data Center using their identifier. To make things easier, we've already provided a data frame with the names and identifiers of each file that could be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>identifier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SWI_2007.tif</td>\n",
              "      <td>urn:uuid:5ee72c9c-789d-4a1c-95d8-cb2b24a20662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SWI_2019.tif</td>\n",
              "      <td>urn:uuid:9cd1cdc3-0792-4e61-afff-c11f86d3a9be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SWI_2021.tif</td>\n",
              "      <td>urn:uuid:14e1e509-77c0-4646-9cc3-d05f8d84977c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SWI_2020.tif</td>\n",
              "      <td>urn:uuid:1ba473ff-8f03-470b-90d1-7be667995ea1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SWI_2001.tif</td>\n",
              "      <td>urn:uuid:85150557-05fd-4f52-8bbd-ec5a2c27e23d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       filename                                     identifier\n",
              "0  SWI_2007.tif  urn:uuid:5ee72c9c-789d-4a1c-95d8-cb2b24a20662\n",
              "1  SWI_2019.tif  urn:uuid:9cd1cdc3-0792-4e61-afff-c11f86d3a9be\n",
              "2  SWI_2021.tif  urn:uuid:14e1e509-77c0-4646-9cc3-d05f8d84977c\n",
              "3  SWI_2020.tif  urn:uuid:1ba473ff-8f03-470b-90d1-7be667995ea1\n",
              "4  SWI_2001.tif  urn:uuid:85150557-05fd-4f52-8bbd-ec5a2c27e23d"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# list of data files\n",
        "# curl -s https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=id:*A2NK3665N+AND+formatType:METADATA\\&fl=identifier,title,documents\\&wt=json | jq .response.docs[].documents[] | sed -e 's/urn:uuid:/\\*/' -e 's/doi:/*/' | xargs -I {} -n 1 curl -s https://arcticdata.io/metacat/d1/mn/v2/query/solr/?fl=identifier,fileName\\&wt=json\\&q=id:{} |jq -r \".response.docs[] | [.fileName, .identifier  | @csv\"\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "TESTDATA=\"\"\"filename,identifier\n",
        "\"SWI_2007.tif\",\"urn:uuid:5ee72c9c-789d-4a1c-95d8-cb2b24a20662\"\n",
        "\"SWI_2019.tif\",\"urn:uuid:9cd1cdc3-0792-4e61-afff-c11f86d3a9be\"\n",
        "\"SWI_2021.tif\",\"urn:uuid:14e1e509-77c0-4646-9cc3-d05f8d84977c\"\n",
        "\"SWI_2020.tif\",\"urn:uuid:1ba473ff-8f03-470b-90d1-7be667995ea1\"\n",
        "\"SWI_2001.tif\",\"urn:uuid:85150557-05fd-4f52-8bbd-ec5a2c27e23d\"\n",
        "\"SWI_trend_2000_2021.tif\",\"urn:uuid:ea99e31b-572a-4175-92d2-0a4a9cdd8366\"\n",
        "\"SWI_2011.tif\",\"urn:uuid:c45e3d93-e908-4f44-85c6-82c4c002a259\"\n",
        "\"SWI_trend_2000_2022.tif\",\"urn:uuid:a0aa394e-b0df-4daf-9011-09bba3c3d0b5\"\n",
        "\"SWI_2015.tif\",\"urn:uuid:5634766f-af81-46f8-acc5-e15f9cd2e3fb\"\n",
        "\"SWI_2017.tif\",\"urn:uuid:94a3fae2-f2f2-4e28-b51b-4a66b7b42331\"\n",
        "\"SWI_2000.tif\",\"urn:uuid:c7aac289-e2be-4be7-85df-6d2fdb5293d5\"\n",
        "\"SWI_2018.tif\",\"urn:uuid:07b9ca95-1463-414a-a3e0-7f466772d891\"\n",
        "\"SWI_2014.tif\",\"urn:uuid:695b949c-339d-4f51-a593-4d332aa86748\"\n",
        "\"SWI_2005.tif\",\"urn:uuid:e54142ed-dbb3-4b27-9756-d2262c6c069a\"\n",
        "\"SWI_2012.tif\",\"urn:uuid:57dcf9f5-2f25-4d37-a793-821edb2dda19\"\n",
        "\"SWI_2008.tif\",\"urn:uuid:75f20002-f4bf-4721-b480-d115d4042e79\"\n",
        "\"resource_map_doi:10.18739_A2NK3665N.rdf\",\"resource_map_doi:10.18739/A2NK3665N\"\n",
        "\"Pan_Arctic_surface_water_yearly_and_trend_over.xml\",\"doi:10.18739/A2NK3665N\"\n",
        "\"SWI_2002.tif\",\"urn:uuid:fe73a6b1-86af-495f-9292-ad22e86a4aa0\"\n",
        "\"SWI_2013.tif\",\"urn:uuid:938856af-d78d-4580-ab63-bb2ce8484c77\"\n",
        "\"SWI_2006.tif\",\"urn:uuid:705a3af3-fe5b-42d3-851f-e6093c15ee5c\"\n",
        "\"SWI_2022.tif\",\"urn:uuid:6c1ea229-8bfa-4e0f-bf6b-cff8e03865fb\"\n",
        "\"SWI_2003.tif\",\"urn:uuid:801e617c-4464-431d-b5c2-a8c16b543e7b\"\n",
        "\"SWI_2016.tif\",\"urn:uuid:bdf983fc-9864-4e08-9d3a-768afe36bf5f\"\n",
        "\"SWI_2009.tif\",\"urn:uuid:e1d9eb59-6e2b-4c0f-b048-206badd03fd1\"\n",
        "\"SWI_2010.tif\",\"urn:uuid:5bf32df3-83bf-4985-b682-560936247516\"\n",
        "\"SWI_2004.tif\",\"urn:uuid:7c1a1730-f1dc-43a7-96c2-fe99d480c87e\"\n",
        "\"\"\"\n",
        "\n",
        "file_table = pd.read_csv(io.StringIO(TESTDATA), sep=\",\")\n",
        "file_table.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Serial\n",
        "\n",
        "When you have a list of repetitive tasks, you may be able to speed it up by adding more computing power.  If each task is completely independent of the others, then it is pleasingly parallel and a prime candidate for executing those tasks in parallel, each on its own core.  For example, let's build a simple loop that downloads the data files that we need for an analysis. First, we start with the serial implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: SWI_2007.tif\n",
            "Downloading: SWI_2019.tif\n",
            "Downloading: SWI_2021.tif\n",
            "Downloading: SWI_2020.tif\n",
            "Downloading: SWI_2001.tif\n",
            "download_serial: 40325.77347755432 ms\n",
            "['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "\n",
        "def download_file(row):\n",
        "    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n",
        "    pid = row[1]['identifier']\n",
        "    filename = row[1]['filename']\n",
        "    url = service + pid\n",
        "    print(\"Downloading: \" + filename)\n",
        "    msg = urllib.request.urlretrieve(url, filename)\n",
        "    return filename\n",
        "\n",
        "@timethis\n",
        "def download_serial(table):\n",
        "    results = [download_file(row) for row in table.iterrows()]\n",
        "    return results\n",
        "    \n",
        "results = download_serial(file_table[0:5])\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this code, we have one function (`download_file`) that downloads a single data file and saves it to disk. It is called iteratively from the function `download_serial`. The serial execution takes about *20-25 seconds*, but can vary considerably based on network traffic and other factors.\n",
        "\n",
        "The issue with this loop is that we execute each download task sequentially, which means that only one of our processors on this machine is in use.  In order to exploit parallelism, we need to be able to dispatch our tasks and allow each to run at the same time, with one task going to each core.  To do that, we can use one of the many parallelization libraries in python to help us out.\n",
        "\n",
        "### Multi-threaded with `concurrent.futures`\n",
        "\n",
        "In this case, we'll use the same `download_file` function from before, but let's switch up and create a `download_threaded()` function to use `concurrent.futures` with a `ThreadPoolExecutor` just as we did earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: SWI_2007.tif\n",
            "Downloading: SWI_2019.tif\n",
            "Downloading: SWI_2021.tif\n",
            "Downloading: SWI_2020.tif\n",
            "Downloading: SWI_2001.tif\n",
            "download_threaded: 27589.443683624268 ms\n",
            "SWI_2007.tif\n",
            "SWI_2019.tif\n",
            "SWI_2021.tif\n",
            "SWI_2020.tif\n",
            "SWI_2001.tif\n"
          ]
        }
      ],
      "source": [
        "#| eval: true\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "@timethis\n",
        "def download_threaded(table):\n",
        "    with ThreadPoolExecutor(max_workers=15) as executor:\n",
        "        results = executor.map(download_file, table.iterrows(), timeout=60)\n",
        "        return results\n",
        "\n",
        "results = download_threaded(file_table[0:5])\n",
        "for result in results:\n",
        "\tprint(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how the \"Downloading...\" messages were printed immediately, but then it still took over 20 seconds to download the 5 files. This could be for several reasons, including that one of the files alone took that long (e.g., due to network congestion), or that there was a bottleneck in writing the files to disk (i.e., we could have been disk I/O limited). Or maybe the multithreaded executor pool didn't do a good job parallelizing the tasks. The trick is figuring out why you did or didn't get a speedup when parallelizing. So, let's try this another way, using a **multi-processing** approach, rather than multi-threading.\n",
        "\n",
        "### Multi-process with `concurrent.futures`\n",
        "\n",
        "You'll remember from earlier that you can execute tasks concurrently by creating multiple threads within one process (multi-threaded), or by creating and executing muliple processes. The latter creates more independence, as each of the executing tasks has their own memory and process space, but it also takes longer to set up. With `concurrent.futures`, we can switch to a multi-process pool by using a `ProcessPoolExecutor`, analogously to how we used `ThreadPoolExecutor` previously. So, some simple changes, and we're running multiple processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: SWI_2021.tifDownloading: SWI_2019.tifDownloading: SWI_2020.tifDownloading: SWI_2007.tifDownloading: SWI_2001.tif\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "download_process: 15851.00245475769 ms\n",
            "SWI_2007.tif\n",
            "SWI_2019.tif\n",
            "SWI_2021.tif\n",
            "SWI_2020.tif\n",
            "SWI_2001.tif\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "@timethis\n",
        "def download_process(table):\n",
        "    with ProcessPoolExecutor(max_workers=15) as executor:\n",
        "        results = executor.map(download_file, table.iterrows(), timeout=60)\n",
        "        return results\n",
        "\n",
        "results = download_process(file_table[0:5])\n",
        "for result in results:\n",
        "\tprint(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, the output messages print almost immediately, but then later the processes finish and report that it took between *10 to 15 seconds* to run. Your mileage may vary. When I increase the number of files being downloaded to 10 or even to 20, I notice it is actually about the same, around *10-15 seconds*.  So, part of our time now is the overhead of setting up multiple processes. But once we have that infrastructure in place, we can make effective euse of the pool of processes to handle many downloads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel processing with `parsl`\n",
        "\n",
        "`concurrent.futures` is great and powerful, but it has its limits. Particularly as you try to scale up into the thousands of concurrent tasks, other libraries like [Parsl](https://parsl-project.org/) ([docs](https://parsl.readthedocs.io/)), [Dask](https://www.dask.org/), [Ray](https://www.ray.io/), and others come into play. They all have their strengths, but Parsl makes it particularly easy to build parallel workflows out of existing python code through it's use of decorators on existing python functions.\n",
        "\n",
        "In addition, Parsl supports a lot of different kinds of [providers](https://parsl.readthedocs.io/en/stable/userguide/execution.html#execution-providers), allowing the same python code to be easily run multi-threaded using a `ThreadPoolExecutor` and via multi-processing on many different cluster computing platforms using the `HighThroughputExecutor`. For example, Parsl includes providers supporting local execution, and on Slurm, Condor, Kubernetes, AWS, and other platforms. And Parsl handles data staging as well across these varied environments, making sure the data is in the right place when it's needed for computations.\n",
        "\n",
        "Similarly to before, we start by configuring an executor in parsl, and loading it. We'll use multiprocessing by configuring the `HighThroughputExecutor` to use our local resources as a cluster, and we'll activate our virtual environment to be sure we're executing in a consistent environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<parsl.dataflow.dflow.DataFlowKernel at 0x7fce141c3f70>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Required packages\n",
        "import parsl\n",
        "from parsl import python_app\n",
        "from parsl.config import Config\n",
        "from parsl.executors import HighThroughputExecutor\n",
        "from parsl.providers import LocalProvider\n",
        "\n",
        "# Configure the parsl executor\n",
        "activate_env = 'workon scomp'\n",
        "htex_local = Config(\n",
        "    executors=[\n",
        "        HighThroughputExecutor(\n",
        "            max_workers_per_node=15,\n",
        "            provider=LocalProvider(\n",
        "                worker_init=activate_env\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "parsl.clear()\n",
        "parsl.load(htex_local)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have a live parsl executor (`htex_local`) that is waiting to execute processes. We tell it to execute processes by annotating functions with decorators that indicate which tasks should be parallelized. Parsl then handles the scheduling and execution of those tasks based on the dependencies between them. In the simplest case, we'll decorate our previous function for downloading a file with the `@python_app` decorator, which tells parsl that any function calls with this function should be run on the default executor (in this case, `htex_local`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create decorated function\n"
          ]
        }
      ],
      "source": [
        "# Decorators seem to be ignored as the first line of a cell, so print something first\n",
        "print(\"Create decorated function\")\n",
        "\n",
        "@python_app\n",
        "def download_file_parsl(row):\n",
        "    import urllib\n",
        "    service = \"https://arcticdata.io/metacat/d1/mn/v2/object/\"\n",
        "    pid = row[1]['identifier']\n",
        "    filename = row[1]['filename']\n",
        "    url = service + pid\n",
        "    print(\"Downloading: \" + filename)\n",
        "    msg = urllib.request.urlretrieve(url, filename)\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we just write regular python code that calls that function, and parsl handles the scheduling. Parsl app executors return an [`AppFuture`](https://parsl.readthedocs.io/en/stable/userguide/futures.html#appfutures), and we can call the `AppFuture.done()` function to determine when the future result is ready without blocking. Or, we can just block on `AppFuture.result()` which waits for each of the executions to complete and then returns the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Define our download_futures function\n",
            "<AppFuture at 0x7fce141c22c0 state=pending>\n",
            "<AppFuture at 0x7fcd6fa7ad10 state=pending>\n",
            "<AppFuture at 0x7fcd6fa7a290 state=pending>\n",
            "<AppFuture at 0x7fcd6fa791e0 state=pending>\n",
            "<AppFuture at 0x7fcd6fa79360 state=pending>\n",
            "download_futures: 15.002727508544922 ms\n",
            "['SWI_2007.tif', 'SWI_2019.tif', 'SWI_2021.tif', 'SWI_2020.tif', 'SWI_2001.tif']\n",
            "wait_for_futures: 13847.8262424469 ms\n"
          ]
        }
      ],
      "source": [
        "#! eval: true\n",
        "print(\"Define our download_futures function\")\n",
        "\n",
        "@timethis\n",
        "def download_futures(table):\n",
        "    futures = []\n",
        "    for row in table.iterrows():\n",
        "        future = download_file_parsl(row)\n",
        "        print(future)\n",
        "        futures.append(future)\n",
        "    return(futures)\n",
        "\n",
        "@timethis\n",
        "def wait_for_futures(table):\n",
        "    futures = download_futures(table)\n",
        "    done = [future.result() for future in futures]\n",
        "    print(done)\n",
        "\n",
        "wait_for_futures(file_table[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we're done, be sure to clean up and shutdown the `htex_local` executor, or it will continue to persist in your environment and utilize resources. Generally, an executor should be created when setting up your environment, and then it can be used repeatedly for many different tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "htex_local.executors[0].shutdown()\n",
        "parsl.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Are we done yet?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parsl and other concurrency libraries generally provide both **blocking** and **non-blocking** methods for accessing the results of asycnchronous method calls. By blocking, we are referring to methods that wait for the asynchronous opearation to complete before returning results, which blocks execution of the rest of a program. By non-blocking, we are referring to methods that return the result if it is available, but not if the async method hasn't completed yet. In that case, it lets the rest of the program to continue execution.\n",
        "\n",
        "In practice this means that we can either 1) wait for all async calls to complete, and then process them using the blocking methods, or 2) query with a non-blocking method to see if each async call is complete, and only then retrieve the results for that method. We illustrate this approach below with parsl."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<parsl.dataflow.dflow.DataFlowKernel at 0x7fcd6fa78dc0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Required packages\n",
        "import parsl\n",
        "from parsl import python_app\n",
        "from parsl.config import Config\n",
        "from parsl.executors import HighThroughputExecutor\n",
        "from parsl.providers import LocalProvider\n",
        "\n",
        "# Configure the parsl executor\n",
        "activate_env = 'workon scomp'\n",
        "htex_local = Config(\n",
        "    executors=[\n",
        "        HighThroughputExecutor(\n",
        "            max_workers_per_node=5,\n",
        "            provider=LocalProvider(\n",
        "                worker_init=activate_env\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "parsl.clear()\n",
        "parsl.load(htex_local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "@python_app\n",
        "def do_stuff(x):\n",
        "    import time\n",
        "    time.sleep(1)\n",
        "    return x**2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<AppFuture at 0x7fcd6f913370 state=pending>\n",
            "<AppFuture at 0x7fcd6f8906d0 state=pending>\n",
            "<AppFuture at 0x7fcd6f9108b0 state=pending>\n",
            "<AppFuture at 0x7fcd6f890310 state=pending>\n",
            "<AppFuture at 0x7fcd6f892410 state=pending>\n",
            "<AppFuture at 0x7fcd6f892470 state=pending>\n",
            "<AppFuture at 0x7fcd6f892d10 state=pending>\n",
            "<AppFuture at 0x7fcd6f891ab0 state=pending>\n",
            "<AppFuture at 0x7fcd6f8905e0 state=pending>\n",
            "<AppFuture at 0x7fcd6f890760 state=pending>\n",
            "Checking:  <AppFuture at 0x7fcd6f913370 state=finished returned int>\n",
            "Do more with result:  0\n",
            "Checking:  <AppFuture at 0x7fcd6f8906d0 state=finished returned int>\n",
            "Do more with result:  1\n",
            "Checking:  <AppFuture at 0x7fcd6f9108b0 state=finished returned int>\n",
            "Do more with result:  4\n",
            "Checking:  <AppFuture at 0x7fcd6f890310 state=finished returned int>\n",
            "Do more with result:  9\n",
            "Checking:  <AppFuture at 0x7fcd6f892410 state=finished returned int>\n",
            "Do more with result:  16\n",
            "Checking:  <AppFuture at 0x7fcd6f892470 state=finished returned int>\n",
            "Do more with result:  25\n",
            "Checking:  <AppFuture at 0x7fcd6f892d10 state=finished returned int>\n",
            "Do more with result:  36\n",
            "Checking:  <AppFuture at 0x7fcd6f891ab0 state=pending>\n",
            "Sorry, come back later.\n",
            "Checking:  <AppFuture at 0x7fcd6f8905e0 state=pending>\n",
            "Sorry, come back later.\n",
            "Checking:  <AppFuture at 0x7fcd6f890760 state=pending>\n",
            "Sorry, come back later.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "all_futures = []\n",
        "for x in range(0,10):\n",
        "    future = do_stuff(x)\n",
        "    all_futures.append(future)\n",
        "    print(future)\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "for future in all_futures:\n",
        "    print(\"Checking: \", future)\n",
        "    if (future.done()):\n",
        "        print(\"Do more with result: \", future.result())\n",
        "    else:\n",
        "        print(\"Sorry, come back later.\")\n",
        "\n",
        "htex_local.executors[0].shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores.  Next, we reviewed the way in which traditional sequential loops can be rewritten as functions that are applied to a list of inputs both serially and in parallel to utilize multiple cores to speed up computations. We reviewed the challenges of optimizing code, where one must constantly examine the bottlenecks that arise as we improve cpu-bound, I/O bound,and memory bound computations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "scomp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
