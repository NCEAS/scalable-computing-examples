{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, set your working directory to the course example folder\n",
    "import os\n",
    "os.chdir(os.path.expanduser('~/scalable-computing-examples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import gcsfs\n",
    "import fsspec\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Zarr dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a path to serve as the root of the Zarr storage hierarchy, and create a root-level group. Verify that something happened to create `data/example.zarr` on the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = zarr.DirectoryStore('data/example.zarr')\n",
    "root = zarr.group(store=store, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add two Zarr groups called `temp` and `precip`. We haven't added any data yet! But what changes on the file system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.create_group('temp')\n",
    "root.create_group('precip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in the `temp` group, let's define a dataset called `t100`. We'll specify the overall shape of this array, the chunk sizes, the data type, and a fill value for missing data ... but no actual data yet! What does this create under `data/example.zarr` on the file system?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.temp.create_dataset('t100',\n",
    "                         shape=(10000, 10000),\n",
    "                         chunks=(1000, 1000),\n",
    "                         dtype='i4',\n",
    "                         fill_value=99,\n",
    "                         overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index on the first 5 rows and columns. Does it look like what you expect? Explain!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within our Python session, now let's create a 10,000 x 10,000 Zarr array of all 1's, specifying a 1K by 1K chunk size. To make it a little more interesting, let's replace the first row to the sequence 0...999. Note that this array only exists in memory for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precip = zarr.ones(shape=(10000, 10000),\n",
    "                   chunks=(1000, 1000),\n",
    "                   dtype='i4')\n",
    "precip[0, :] = np.arange(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index on the first 5 rows and columns. Does it look like what you expect?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add this precip array to our file-based Zarr store, with the name `p100`. Then look again at the file system. What do you now see under the precip group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.precip['p100'] = precip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zarr objects in Python have a useful `.info()` method that you can use to retrieve useful operational details about the array (or group). Use this on the stored `t100` and `p100` arrays, as well as your in-memory `precip` array, and consider what it tells you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the .info() method on Zarr objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back to our t100 array, and assign 10 across the last\n",
    "# 3 row by 3 column subarray of the data\n",
    "\n",
    "# ... then index on the last 5 rows and columns. Does it looks like what you expect?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what has changed under example.zarr on the file system? Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, close the file store connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can read our newly created Zarr store using the `zarr` library, and inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `zarr.open(<path>)` to read in your newly created Zarr store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the `tree()` method to visualize its structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the precip/p100 array look like what you created?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve CMIP6 data from a remote Zarr store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start out by reading a CSV file that catalogs all the available CMIP6 Zarr stores in this Google Cloud Storage account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row identifies a Zarr stores. How many are there in total?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the first few rows look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's use pandas to select CMIP zstore records corresponding to a simulation of the recent past (`historical`) from the ocean daily (`Oday`) table, focusing on the sea surface height (`tos`) variable. Weâ€™ll also only select results from the NOAA Geophysical Fluid Dynamics Laboratory (`NOAA-GFDL`) runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ta = df.query(\"\"\"\n",
    "    activity_id=='CMIP' &\n",
    "    table_id == 'Oday' &\n",
    "    variable_id == 'tos' &\n",
    "    experiment_id == 'historical' &\n",
    "    institution_id == 'NOAA-GFDL'\n",
    "    \"\"\".replace('\\n', ''))\n",
    "df_ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are sorted in version order, so we'll just take the final record, and retrieve the Zarr store path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zstore = df_ta.zstore.values[-1]\n",
    "print(f'zstore: {zstore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Zarr, we need to set up a MutableMapping interface to the storage system.\n",
    "\n",
    "We could use the gcsfs library for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "#mapper = gcs.get_mapper(zstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But instead let's introduce `fsspec`, a useful library that abstracts over many kinds of local and remote connections. Here it detects that we're connecting to GCS, and internally uses the right implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(zstore, token='anon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: To get a quick sense of what `fsspec` supports, run these two lines in a code cell:\n",
    "\n",
    "```python\n",
    "from fsspec.registry import known_implementations\n",
    "{k: v.get('class') for k, v in known_implementations.items()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use xarray to reach into the Zarr store.\n",
    "\n",
    "Note that this will use Dask by default (if Dask is installed), automagically giving us a lazy, chunked representation of the data See [the docs](https://docs.xarray.dev/en/stable/user-guide/dask.html#reading-and-writing-data) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the ds dataset, and specifically the 'tos' data array\n",
    "# Notice the dimensionality and size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a tiny piece of the data, taking a lat slice of 71 to 73,\n",
    "# lon slice of 203 to 205, and time slice of 2010 to 2012\n",
    "\n",
    "\n",
    "# How big is this subset of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create a pair of smaller summary datasets, first taking the mean for each day across our returned spatial grid cells, and then taking a 90-day rolling mean over the daily time series.\n",
    "\n",
    "Importantly, note that because we're working with Dask arrays, these are all lazy computations! We haven't loaded any data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sst = tos_subset.mean(dim=('lat', 'lon'))\n",
    "rolling_90d_sst = daily_sst.rolling(time=90, center=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a plot, which will force execution and therefore loading of actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sst.plot(label=\"daily\")\n",
    "rolling_90d_sst.plot(label=\"rolling annual mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, you can try adding `.load()` at the end of the assignments above where we first calculated the daily mean and rolling mean, then try plotting again. Notice the speedup in plotting after we've pre-loaded the data in `daily_sst` and `rolling_90d_sst`. In this case, because those summary datasets are so small, this is safe and convenient, especially if we end up re-plotting many times to get the aesthetic details right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's try writing our small dataset out to a local Zarr store, just to see what it looks like on disk, then read it back in to verify everything is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our original subset out to a local Zarr store\n",
    "tos_subset.to_zarr('data/cmip_tos_subset.zarr', consolidated=True)\n",
    "\n",
    "# ... then open it again using xarray\n",
    "xr.open_dataset('data/cmip_tos_subset.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUR SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one more Zarr example, this time pulling SST data from an AWS S3 bucket. We'll use `fsspec` again. Note how similar this is to the code we used to connect to the CMIP6 data in GCS above.\n",
    "\n",
    "One difference is that here we chose to use the generic `xr.open_dataset()` function rather than `xr.open_zarr()`. Functionally these do the same thing, but the general development trajectory for `xarray` is to favor use of the higher level `open_dataset()` function over the lower-level format-specific functions. When calling this function, we can optionally control Dask chunking using `chunks` argument; here we pass in the value `{}`, which tells `open_dataset()` to choose a chunk layout that preferred by the Zarr engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In case your don't have `s3fs` installed in your virtual environment,\n",
    "you can install it from within a Jupyter cell with the following command:\n",
    "```python\n",
    "!pip install s3fs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = 's3://mur-sst/zarr'\n",
    "\n",
    "mapper = fsspec.get_mapper(file_location, anon=True)\n",
    "ds_sst = xr.open_dataset(mapper, engine='zarr', consolidated=True, chunks={})\n",
    "\n",
    "# check out the size of this Zarr array!\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a 1-D time series of the sea_ice_fraction variable array,\n",
    "# extracting lat 73 and lon -157\n",
    "\n",
    "\n",
    "# How big is this subset of data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a quick time series plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_ice_ts.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get a chunk of sea_ice_fraction for the year 2015,\n",
    "# slicing on lat 72.5 to 73, and lon -157.5 to -157\n",
    "\n",
    "# How big is this subset of data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our subset of data from 2015, calculate monthly mean sea ice fraction for all grid cells in this chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monthly_mean_ice = sea_ice_chunk.groupby(\"time.month\").mean()\n",
    "monthly_mean_ice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a faceted plot visualizing the the ice area fraction by month across our little swath of spatial grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "fg = monthly_mean_ice.plot(\n",
    "    col=\"month\",\n",
    "    col_wrap=4,\n",
    "    cmap=mpl.cm.RdYlBu\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
